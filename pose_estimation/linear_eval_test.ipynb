{"cells":[{"cell_type":"code","execution_count":null,"id":"9a8784ab","metadata":{},"outputs":[],"source":["all_connections = {\n","    \"panoptic\": [\n","        (7, 8),\n","        (6, 7),\n","        (2, 6),\n","        (2, 12),\n","        (12, 13),\n","        (13, 14),\n","        (2, 0),\n","        (0, 3),\n","        (0, 9),\n","        (3, 4),\n","        (4, 5),\n","        (9, 10),\n","        (10, 11),\n","        (0, 16),\n","    ],\n","    \"skiPose\": [\n","        [0, 1],\n","        [1, 2],\n","        [2, 3],\n","        [0, 4],\n","        [4, 5],\n","        [5, 6],\n","        [8, 9],\n","        [9, 10],\n","        [8, 11],\n","        [11, 12],\n","        [12, 13],\n","        [0, 7],\n","        [14, 15],\n","        [15, 16],\n","        [7, 8],\n","        [14, 8],\n","    ],\n","}\n","\n","import sys\n","\n","sys.path.append(\"..\")\n","\n","from pose_estimation.models import getPoseEstimModel\n","from pose_estimation.utils import getDatasetLoader, getLatestModel\n","from dataloaders.datasets import out_joints, pose_datasets\n","import torch\n","from torchvision.models.resnet import ResNet50_Weights, ResNet18_Weights\n","import torchvision.transforms as T\n","\n","\n","dataset = \"skiPose\"\n","models_out = [\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","    \"skiPose\",\n","]\n","datasets_dir = \"../datasets\"\n","models = [\n","    \"../trained_models/simsiam50SkiPan.25_estim_ski\",\n","    \"../trained_models/simsiam50SkiPan.25_estim_ski_1\",\n","    \"../trained_models/simsiam50SkiPan.5_estim_ski\",\n","    \"../trained_models/simsiam50SkiPan.5_estim_ski_1\",\n","    \"../trained_models/simclr50SkiPan.25_estim_ski\",\n","    \"../trained_models/simclr50SkiPan.25_estim_ski_1\",\n","    \"../trained_models/simclr50SkiPan.5_estim_ski\",\n","    \"../trained_models/simclr50SkiPan.5_estim_ski_1\",\n","    \"../trained_models/resnet50_estim_ski_JE\",\n","    \"../trained_models/resnet50_estim_ski_JE_1\",\n","]\n","names = [\n","    \"simsiam 25% Panoptic\",\n","    \"simsiam 25% Panoptic LN + 1 layer\",\n","    \"simsiam 50% Panoptic\",\n","    \"simsiam 50% Panoptic LN + 1 layer\",\n","    \"simclr 25% Panoptic\",\n","    \"simclr 25% Panoptic LN + 1 layer\",\n","    \"simclr 50% Panoptic\",\n","    \"simclr 50% Panoptic LN + 1 layer\",\n","    \"resnet50 JE\",\n","    \"resnet50 JE + LN + 1 layer\",\n","]\n","backbones = [\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","    \"resnet50\",\n","]\n","architectures = [\n","    [],\n","    [128],\n","    [],\n","    [128],\n","    [],\n","    [128],\n","    [],\n","    [128],\n","    [],\n","    [128],\n","]\n","activations = [\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","    \"relu\",\n","]\n","out_dim = [out_joints[model_out] * 3 for model_out in models_out]\n","LN = [\n","    False,\n","    True,\n","    False,\n","    True,\n","    False,\n","    True,\n","    False,\n","    True,\n","    False,\n","    True,\n","    True,\n","    True,\n","]\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","test_data = {}\n","\n","transforms = ResNet18_Weights.DEFAULT.transforms()\n","transforms = T.Compose([T.ToPILImage(), transforms])\n","_, _, test_data[\"resnet18\"] = pose_datasets[dataset](transforms, datasets_dir)\n","\n","\n","transforms = ResNet50_Weights.DEFAULT.transforms()\n","transforms = T.Compose([T.ToPILImage(), transforms])\n","_, _, test_data[\"resnet50\"] = pose_datasets[dataset](transforms, datasets_dir)\n","\n","clear_models = []\n","for i, model in enumerate(models):\n","    print(\"loading model: \", model)\n","    clear_model = getPoseEstimModel(\n","        \"\",\n","        \"resnet\",\n","        architectures[i],\n","        out_dim[i],\n","        device,\n","        backbones[i],\n","        LN[i],\n","        activations[i],\n","    )\n","    model_checkpoint = getLatestModel(model)\n","    print(\"\\tloading model from: \", model_checkpoint)\n","    clear_model.load_state_dict(\n","        torch.load(model_checkpoint, map_location=torch.device(device))\n","    )\n","    clear_models.append(clear_model)\n","    print(\"\\tmodel loaded\")\n","\n","# print(clear_model1 == clear_model2)\n","if device == \"cpu\":\n","    for model in clear_models:\n","        model = model.module"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QcqrDPrW0kbu"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from pose_estimation.functions import find_rotation_mat  # As per your instruction\n","from scipy.spatial.transform import Rotation as R\n","from pose_estimation.functions import get_loss\n","\n","\n","def plot_models(\n","    models,\n","    backbones,\n","    index,\n","    all_connections,\n","    test_data,\n","    dataset,\n","    model_outs,\n","    model_names,\n","    show_image=True,\n","    models_to_plot=None,\n","):\n","    \"\"\"\n","    Plots predictions from multiple models against a single ground truth,\n","    with each pose normalized individually to the range [0, 1] while maintaining proportions.\n","    Aligns each pose to be upright (perpendicular to the floor) and facing the viewer.\n","    Each pose is plotted in a separate subplot for clarity.\n","\n","    Args:\n","        models (list): A list of trained pose estimation models.\n","        backbones (list): A list of backbone names corresponding to each model.\n","        index (int): The index of the data sample to visualize.\n","        all_connections (dict): A dictionary mapping dataset names to their joint connections.\n","        test_data (dict): A dictionary containing test datasets mapped by backbone names.\n","        dataset (str): The name of the dataset to use for plotting (determines the connections for ground truth).\n","        model_outs (list): A list of dataset names corresponding to each model's output format, used to get connections.\n","        model_names (list): A list of names for each model, used in plot titles and legends.\n","        show_image (bool): Whether to display the original image alongside the 3D plots.\n","        models_to_plot (list or None): List of indices of models to plot. If None, all models are plotted.\n","\n","    Raises:\n","        ValueError: If the lengths of models, backbones, model_outs, and model_names do not match.\n","        ValueError: If the specified dataset is not found in all_connections.\n","    \"\"\"\n","    # Validate input lengths\n","    if not (len(models) == len(backbones) == len(model_outs) == len(model_names)):\n","        raise ValueError(\n","            \"The lengths of models, backbones, model_outs, and model_names must be equal.\"\n","        )\n","\n","    # Validate dataset\n","    if dataset not in all_connections:\n","        raise ValueError(f\"Dataset '{dataset}' not found in all_connections.\")\n","\n","    num_models = len(models)\n","\n","    # Handle models_to_plot\n","    if models_to_plot is None:\n","        models_to_plot = list(range(num_models))\n","    else:\n","        # Validate models_to_plot\n","        if not all(0 <= idx < num_models for idx in models_to_plot):\n","            raise ValueError(\"models_to_plot contains invalid model indices.\")\n","\n","    # Collect ground truth pose and image from the first backbone's test data\n","    first_backbone = backbones[0]\n","    if first_backbone not in test_data:\n","        raise ValueError(f\"Test data key '{first_backbone}' not found in test_data.\")\n","\n","    ground_sample = test_data[first_backbone][index]\n","    ground_image = ground_sample[\"image\"]\n","    ground_coords = ground_sample[\"poses_3d\"].reshape((-1, 3))\n","    device = next(models[0].parameters()).device\n","    ground_coords_tensor = torch.tensor(ground_coords, dtype=torch.float32).to(device)\n","\n","    # Align and normalize ground truth pose\n","    ground_coords_aligned = align_pose(ground_coords_tensor, dataset)\n","    ground_coords_normalized = normalize_pose(ground_coords_aligned)\n","\n","    # Determine subplot layout\n","    total_plots = (\n","        len(models_to_plot) + (1 if show_image else 0) + 1\n","    )  # +1 for ground truth\n","    max_cols = 4\n","    ncols = min(total_plots, max_cols)\n","    nrows = (total_plots + max_cols - 1) // max_cols  # Ceiling division\n","    fig = plt.figure(figsize=(5 * ncols, 5 * nrows))\n","\n","    plot_idx = 1\n","\n","    # Plot ground truth\n","    gt_connections = all_connections[dataset]\n","    ax_gt = fig.add_subplot(nrows, ncols, plot_idx, projection=\"3d\")\n","    plot_pose(\n","        ax_gt, ground_coords_normalized, gt_connections, title=\"Ground Truth\", color=\"b\"\n","    )\n","    plot_idx += 1\n","\n","    # Plot each model's prediction in separate subplots\n","    losses = {}\n","    for i in models_to_plot:\n","        model = models[i]\n","        backbone = backbones[i]\n","        model_out = model_outs[i]\n","        model_name = model_names[i]\n","        pred_aligned, pred = get_model_prediction(\n","            model, backbone, test_data, index, model_out\n","        )\n","        loss = get_loss(\n","            pred.unsqueeze(0),\n","            torch.tensor(ground_sample[\"poses_3d\"]).unsqueeze(0).to(pred.device),\n","        )\n","        pred_normalized = normalize_pose(pred_aligned)\n","        model_connections = all_connections[model_out]\n","        ax_model = fig.add_subplot(nrows, ncols, plot_idx, projection=\"3d\")\n","        plot_pose(\n","            ax_model,\n","            pred_normalized,\n","            model_connections,\n","            title=model_name,\n","            color=\"r\",\n","            loss=loss,\n","        )\n","        plot_idx += 1\n","\n","        losses[model_name] = loss\n","\n","    # Plot the image if required\n","    if show_image:\n","        ax_img = fig.add_subplot(nrows, ncols, plot_idx)\n","        plot_image(ax_img, ground_image)\n","        plot_idx += 1\n","\n","    # sort the losses\n","    losses = dict(sorted(losses.items(), key=lambda item: item[1]))\n","    print(\"Ranking:\")\n","    for i, model in enumerate(losses):\n","        print(f'\\t{i+1} - \"{model}\" Loss {losses[model]}')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def normalize_pose(pose):\n","    \"\"\"Normalizes a single pose to [0, 1] range while maintaining proportions.\"\"\"\n","    # Convert to NumPy array if it's a torch tensor\n","    if isinstance(pose, torch.Tensor):\n","        pose = pose.cpu().numpy()\n","    elif isinstance(pose, np.ndarray):\n","        pass\n","    else:\n","        raise TypeError(\"Pose must be a torch.Tensor or numpy.ndarray.\")\n","\n","    min_coords = pose.min(axis=0)\n","    max_coords = pose.max(axis=0)\n","    range_coords = max_coords - min_coords\n","    max_range = range_coords.max()\n","    if max_range == 0:\n","        normalized = pose - min_coords  # Avoid division by zero\n","    else:\n","        normalized = (pose - min_coords) / max_range\n","    return normalized\n","\n","\n","def get_model_prediction(model, backbone, test_data, index, model_out):\n","    \"\"\"Gets and processes the prediction from a single model.\"\"\"\n","    device = next(model.parameters()).device\n","    sample = test_data[backbone][index]\n","    image = sample[\"image\"]\n","    image_tensor = image.unsqueeze(0).to(device)\n","\n","    # Model Prediction\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        pred_model = model(\n","            image_tensor\n","        )  # Assuming model outputs (batch_size, num_joints*3)\n","        pred = pred_model[0].reshape((-1, 3)).detach()\n","        pred_aligned = align_pose(pred, model_out)\n","        pred_np = pred_aligned.cpu().numpy()\n","\n","    return pred_np, pred_model\n","\n","\n","def align_pose(pose_tensor, dataset_name):\n","    \"\"\"\n","    Aligns the pose to be upright (perpendicular to the floor) and facing the viewer.\n","\n","    Args:\n","        pose_tensor (torch.Tensor): Pose tensor of shape (N, 3).\n","        dataset_name (str): The name of the dataset to determine joint indices.\n","\n","    Returns:\n","        torch.Tensor: Aligned pose tensor of shape (N, 3).\n","    \"\"\"\n","    # Center the pose around its mean\n","    pose_centered = pose_tensor - pose_tensor.mean(dim=0)\n","\n","    # Define joint indices based on the dataset\n","    # Adjust these indices according to your dataset's joint definitions\n","    if dataset_name == \"panoptic\":\n","        hip_index = 2\n","        head_index = 14\n","        left_shoulder_index = 9\n","        right_shoulder_index = 3\n","    elif dataset_name == \"skiPose\":\n","        hip_index = 0\n","        head_index = 14\n","        left_shoulder_index = 5\n","        right_shoulder_index = 2\n","    else:\n","        # Default indices\n","        hip_index = 0\n","        head_index = -1\n","        left_shoulder_index = 5\n","        right_shoulder_index = 6\n","\n","    # Compute the \"up\" vector (from hips to head)\n","    if hip_index < len(pose_centered) and head_index < len(pose_centered):\n","        up_vector = pose_centered[head_index] - pose_centered[hip_index]\n","    else:\n","        up_vector = torch.tensor(\n","            [0, 0, 1], dtype=pose_tensor.dtype, device=pose_tensor.device\n","        )\n","\n","    # Normalize the up vector\n","    up_vector_norm = torch.norm(up_vector)\n","    if up_vector_norm == 0:\n","        up_vector = torch.tensor(\n","            [0, 0, 1], dtype=pose_tensor.dtype, device=pose_tensor.device\n","        )\n","    else:\n","        up_vector = up_vector / up_vector_norm\n","\n","    # Desired up vector (aligned with Z-axis)\n","    target_up = torch.tensor(\n","        [0, 0, 1], dtype=pose_tensor.dtype, device=pose_tensor.device\n","    )\n","\n","    # Compute rotation to align up_vector with target_up\n","    rotation_mat = compute_alignment_rotation(up_vector, target_up)\n","\n","    # Apply rotation\n","    pose_aligned = torch.matmul(pose_centered, rotation_mat.T)\n","\n","    # Rotate around Z-axis to face the viewer\n","    # Compute the forward vector (from left shoulder to right shoulder)\n","    if left_shoulder_index < len(pose_aligned) and right_shoulder_index < len(\n","        pose_aligned\n","    ):\n","        forward_vector = (\n","            pose_aligned[right_shoulder_index] - pose_aligned[left_shoulder_index]\n","        )\n","    else:\n","        forward_vector = torch.tensor(\n","            [1, 0, 0], dtype=pose_tensor.dtype, device=pose_tensor.device\n","        )\n","\n","    # Project forward_vector onto XY-plane\n","    forward_vector_xy = forward_vector.clone()\n","    forward_vector_xy[2] = 0\n","    forward_vector_xy_norm = torch.norm(forward_vector_xy)\n","    if forward_vector_xy_norm == 0:\n","        forward_vector_xy = torch.tensor(\n","            [1, 0, 0], dtype=pose_tensor.dtype, device=pose_tensor.device\n","        )\n","    else:\n","        forward_vector_xy = forward_vector_xy / forward_vector_xy_norm\n","\n","    # Desired forward direction (Y-axis)\n","    target_forward = torch.tensor(\n","        [0, 1, 0], dtype=pose_tensor.dtype, device=pose_tensor.device\n","    )\n","\n","    # Compute rotation around Z-axis to align forward_vector_xy with target_forward\n","    angle_z = torch.atan2(forward_vector_xy[1], forward_vector_xy[0]) - torch.atan2(\n","        target_forward[1], target_forward[0]\n","    )\n","    cos_angle = torch.cos(-angle_z)\n","    sin_angle = torch.sin(-angle_z)\n","    rotation_z = torch.tensor(\n","        [[cos_angle, -sin_angle, 0], [sin_angle, cos_angle, 0], [0, 0, 1]],\n","        dtype=pose_tensor.dtype,\n","        device=pose_tensor.device,\n","    )\n","\n","    # Apply rotation around Z-axis\n","    pose_aligned = torch.matmul(pose_aligned, rotation_z.T)\n","\n","    return pose_aligned\n","\n","\n","def compute_alignment_rotation(source_vector, target_vector):\n","    \"\"\"\n","    Computes the rotation matrix that aligns source_vector to target_vector.\n","\n","    Args:\n","        source_vector (torch.Tensor): Source vector of shape (3,).\n","        target_vector (torch.Tensor): Target vector of shape (3,).\n","\n","    Returns:\n","        torch.Tensor: Rotation matrix of shape (3, 3).\n","    \"\"\"\n","    v = torch.cross(source_vector, target_vector)\n","    c = torch.dot(source_vector, target_vector)\n","    s = torch.norm(v)\n","\n","    if s == 0:\n","        # Vectors are parallel\n","        if c > 0:\n","            # Same direction\n","            return torch.eye(3, dtype=source_vector.dtype, device=source_vector.device)\n","        else:\n","            # Opposite direction\n","            # Rotate 180 degrees around any orthogonal axis\n","            orthogonal_axis = torch.tensor(\n","                [1, 0, 0], dtype=source_vector.dtype, device=source_vector.device\n","            )\n","            if torch.allclose(source_vector, orthogonal_axis):\n","                orthogonal_axis = torch.tensor(\n","                    [0, 1, 0], dtype=source_vector.dtype, device=source_vector.device\n","                )\n","            v = torch.cross(source_vector, orthogonal_axis)\n","            v = v / torch.norm(v)\n","            return rotation_matrix_from_axis_angle(v, torch.pi)\n","    else:\n","        kmat = torch.tensor(\n","            [[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]],\n","            dtype=source_vector.dtype,\n","            device=source_vector.device,\n","        )\n","        rotation_matrix = (\n","            torch.eye(3, dtype=source_vector.dtype, device=source_vector.device)\n","            + kmat\n","            + kmat @ kmat * ((1 - c) / (s**2))\n","        )\n","        return rotation_matrix\n","\n","\n","def rotation_matrix_from_axis_angle(axis, angle):\n","    \"\"\"\n","    Computes the rotation matrix from an axis and angle using Rodrigues' rotation formula.\n","\n","    Args:\n","        axis (torch.Tensor): Rotation axis of shape (3,).\n","        angle (float): Rotation angle in radians.\n","\n","    Returns:\n","        torch.Tensor: Rotation matrix of shape (3, 3).\n","    \"\"\"\n","    K = torch.tensor(\n","        [[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]],\n","        dtype=axis.dtype,\n","        device=axis.device,\n","    )\n","\n","    R = (\n","        torch.eye(3, dtype=axis.dtype, device=axis.device)\n","        + torch.sin(angle) * K\n","        + (1 - torch.cos(angle)) * torch.matmul(K, K)\n","    )\n","    return R\n","\n","\n","def plot_pose(ax, pose, connections, title=\"\", color=\"r\", loss=0):\n","    \"\"\"Plots a single pose in a 3D subplot.\"\"\"\n","    ax.scatter(pose[:, 0], pose[:, 1], pose[:, 2], c=color)\n","    for connection in connections:\n","        start, end = connection\n","        if start < len(pose) and end < len(pose):\n","            ax.plot(\n","                [pose[start, 0], pose[end, 0]],\n","                [pose[start, 1], pose[end, 1]],\n","                [pose[start, 2], pose[end, 2]],\n","                c=color,\n","            )\n","    # write name and loss (in scientific notation)\n","    ax.set_title(f\"{title} - Error {loss:.2E}\")\n","    ax.set_xlabel(\"X\")\n","    ax.set_ylabel(\"Y\")\n","    ax.set_zlabel(\"Z\")\n","    ax.set_xlim(0, 1)\n","    ax.set_ylim(0, 1)\n","    ax.set_zlim(0, 1)\n","    ax.view_init(elev=20, azim=180)  # Adjust view angle for better visualization\n","\n","\n","def plot_image(ax, image_tensor):\n","    \"\"\"Plots the image in a subplot.\"\"\"\n","    # Convert image tensor to numpy array\n","    img = image_tensor.clone().detach().cpu()\n","    if img.shape[0] == 1:\n","        img = img.squeeze(0)  # Remove channel dimension for grayscale\n","        img = img.numpy()\n","        cmap = \"gray\"\n","    else:\n","        img = img.permute(1, 2, 0).numpy()\n","        cmap = None  # RGB image\n","\n","    # Ensure image values are in [0,1]\n","    img = img - img.min()\n","    if img.max() != 0:\n","        img = img / img.max()\n","\n","    ax.imshow(img, cmap=cmap)\n","    ax.set_title(\"Original Image\")\n","    ax.axis(\"off\")"]},{"cell_type":"code","execution_count":3,"id":"d76c7ec7","metadata":{},"outputs":[],"source":["# generate unique indexes\n","indexes = np.random.choice(len(test_data[\"resnet50\"]), 10, replace=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for index in indexes:\n","\n","    print(\"________________________________________________________\")\n","\n","    print(\"Index: \", index)\n","\n","    plot_models(\n","        clear_models,\n","        backbones,\n","        index,\n","        all_connections,\n","        test_data,\n","        dataset,\n","        models_out,\n","        names,\n","        show_image=True,\n","        models_to_plot=None,\n","    )"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}
