{"cells":[{"cell_type":"code","execution_count":null,"id":"9a8784ab","metadata":{},"outputs":[],"source":["connections = [(7,8),(6,7),(2,6),(2,12),(12,13),(13,14),(2,0),(0,3),(0,9),(3,4),(4,5),(9,10),(10,11),(0,16)]"]},{"cell_type":"code","execution_count":null,"id":"09542507","metadata":{},"outputs":[],"source":["from torchvision.models import resnet50, ResNet50_Weights\n","import torch.nn as nn\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, out_dim):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, out_dim),\n","        )\n","\n","    def forward(self, x):\n","        return x, self.layers(x)\n","\n","def get_simclr_net():\n","    weights = ResNet50_Weights.DEFAULT\n","    model = resnet50(weights=weights)\n","    model.fc = MLP(2048, 2048, 128)\n","\n","    return model\n","\n","class Projector(nn.Module):\n","    def __init__(self, input_dim, out_dim, hidder_proj):\n","        super().__init__()\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_dim, hidder_proj),\n","            nn.BatchNorm1d(hidder_proj),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(hidder_proj, hidder_proj),\n","            nn.BatchNorm1d(hidder_proj),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(hidder_proj, out_dim),\n","        )\n","\n","    def forward(self, x):\n","        return x, self.proj(x)\n","\n","class SiamMLP(nn.Module):\n","    def __init__(self, base, input_dim, out_dim, hidder_proj, hidden_pred):\n","        super().__init__()\n","\n","        self.base = base\n","\n","        base.fc = Projector(input_dim, out_dim, hidder_proj)\n","\n","        self.predictor = nn.Sequential(\n","            nn.Linear(out_dim, hidden_pred),\n","            nn.BatchNorm1d(hidden_pred),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(hidden_pred, out_dim)\n","        )\n","\n","\n","    def forward(self, x):\n","        x, projections =  self.base(x)\n","\n","        predictions = self.predictor(projections)\n","\n","        return x, projections.detach(), predictions\n","\n","def get_siam_net():\n","    weights = ResNet50_Weights.DEFAULT\n","    model = resnet50(weights=weights)\n","    model = SiamMLP(model, 2048, 2048, 2048, 512)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1VYaEOcUoGcVV-4WXfc-uivNn0_i4hLYh"},"id":"2fb45b47","outputId":"f9b5a508-3060-4708-b126-40035c0c677a"},"outputs":[],"source":["import os\n","import torch\n","import math\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import json\n","import cv2\n","import re\n","import random\n","import torchvision.transforms as transforms\n","from torchvision.io import read_image\n","\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as T\n","\n","\n","\n","class PanopticDataset(Dataset):\n","    def __init__(self, transform):\n","\n","        # change this to the path where the dataset is stored\n","        self.data_path = \"ProcessedPanopticDataset\\\\\"\n","        self.training_dir = []\n","\n","        self.transform = transform\n","\n","        paths = []\n","\n","        motion_seq = os.listdir(self.data_path)\n","        no_dir = ['scripts','python','matlab','.git','glViewer.py','README.md','matlab',\n","                'README_kinoptic.md']\n","\n","        for dir in motion_seq:\n","            if dir not in no_dir:\n","                if 'haggling' in dir:\n","                    continue\n","                elif dir == '171204_pose2' or dir =='171204_pose5' or dir =='171026_cello3':\n","                    if os.path.exists(os.path.join(self.data_path,dir,'hdJoints')):\n","                        data_path = os.path.join(self.data_path,dir,'hdJoints')\n","                        for lists in (os.listdir(data_path)):\n","                            paths.append(os.path.join(data_path,lists.split('.json')[0]))\n","                elif 'ian' in dir:\n","                    continue\n","                else:\n","                    if os.path.exists(os.path.join(self.data_path,dir,'hdJoints')):\n","                        data_path = os.path.join(self.data_path,dir,'hdJoints')\n","                        for lists in (os.listdir(data_path)):\n","                            \n","                            paths.append(os.path.join(data_path,lists.split('.json')[0]))\n","\n","        self.data = {'paths': paths}\n","\n","    def __len__(self):\n","        return len(self.data['paths'])\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        sample = dict()\n","\n","        path_split = self.data['paths'][idx].split('\\\\hdJoints')\n","        image_path = path_split[0] + '\\\\hdImages' + path_split[-1] + '.jpg'\n","        sample['image_path'] = image_path\n","\n","        image = cv2.imread(image_path)\n","        image =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = self.transform(image)\n","\n","        sample['image'] = image\n","\n","        joints_path = self.data['paths'][idx]+'.json'\n","\n","        with open(joints_path) as dfile:\n","            bframe = json.load(dfile)\n","\n","        poses_3d = torch.tensor(np.array(bframe['poses_3d']), dtype=torch.float32)\n","        #remove every 4th element of one dimensional array poses_3d\n","        poses_3d = poses_3d.reshape(-1, 4)[:, :3].reshape(-1)\n","\n","        sample['poses_3d'] =  poses_3d\n","\n","        cam = bframe['cam']\n","        # Replace 'array' with 'list' in the cam string\n","        cam = cam.replace('array', 'list')\n","\n","        sample['cam'] = cam\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71ffc15a"},"outputs":[],"source":["# uncomment this if you get an No module named 'pytorch_lightning.utilities.apply_func' error\n","# !pip3 install pytorch-lightning==1.2.2"]},{"cell_type":"markdown","metadata":{"id":"abacd64a"},"source":["### On top of a base encoder, a simple linear layer is attached that predicts 19 pairs of joints. The base encoder can be either SimCLR or Siamese models. I remove the projection head (or projection and prediction in case of Siamese) and use only the output of the base encoder, which is the output of AvrPool layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76e123aa"},"outputs":[],"source":["class Linear(nn.Module):\n","    def __init__(self):\n","        super(Linear, self).__init__()\n","        #initialize weights and biases to 0\n","        self.layers = nn.Sequential(nn.Linear(2048, 512), nn.ReLU(), nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 18*3))\n","\n","    def forward(self, x):\n","        z = self.layers(x)\n","        return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d379cd49"},"outputs":[],"source":["def get_linear_evaluation_model(path, base, siam=True):\n","\n","    base.load_state_dict(torch.load(path, map_location=torch.device('cuda')))\n","\n","    if siam:\n","        base = base.base\n","        base.fc = Linear()\n","    else:\n","        base.fc = Linear()\n","\n","    return base"]},{"cell_type":"markdown","metadata":{"id":"9780c064"},"source":["### I freeze all the layers of the base encoder and use Stochastic gradient descent to train only the linear layer."]},{"cell_type":"markdown","metadata":{"id":"d8e81d26"},"source":["### The images are resized into (128, 128) since the models were trained using this size. The dataset is split into 60%/20%/20% for training, validation and testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3e436a09"},"outputs":[],"source":["generator = torch.Generator().manual_seed(42)\n","\n","def get_data(batch_size):\n","\n","    transforms = T.Compose(\n","        [\n","            T.ToTensor(),\n","            T.Resize(size=(128, 128)),\n","        ]\n","    )\n","\n","    data = PanopticDataset(transforms)\n","\n","    num_samples = len(data)\n","\n","    training_samples = int(num_samples * 0.6 + 1)\n","    val_samples = int(num_samples * 0.2 + 1)\n","    test_samples = num_samples - training_samples - val_samples\n","\n","    training_data, val_data, test_data = torch.utils.data.random_split(\n","        data, [training_samples, val_samples, test_samples], generator=generator\n","    )\n","\n","    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n","\n","    return training_data, val_data, test_data, train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a423fabc"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Assuming your get_data function returns appropriate data\n","training_data, _, _, _, _, _ = get_data(201)\n","\n","image = training_data[9]['image']\n","coords = training_data[9]['poses_3d'].reshape((-1, 3))\n","cam = training_data[9]['cam']\n","\n","\n","def project_points( points_3d, camera_parameters_str):\n","    # Load camera parameters\n","    camera_params = eval(camera_parameters_str)\n","\n","    # Camera intrinsic matrix\n","    K = camera_params['K']\n","\n","    # Rotation matrix and translation vector\n","    R = camera_params['R']\n","    t = camera_params['t']\n","\n","    # Projection matrix\n","    P = np.dot(K, np.hstack((R, t)))\n","\n","    # Homogeneous 3D points\n","    points_3d_homogeneous = np.hstack((points_3d, np.ones((points_3d.shape[0], 1))))\n","\n","    # Project 3D points to 2D\n","    points_2d_homogeneous = np.dot(P, points_3d_homogeneous.T).T\n","\n","    # Normalize homogeneous coordinates\n","    points_2d = points_2d_homogeneous[:, :2] / points_2d_homogeneous[:, 2:]\n","\n","    # rseize to 128x128\n","    points_2d[:, 0] = points_2d[:, 0] * 128 / 1080\n","    points_2d[:, 1] = points_2d[:, 1] * 128 / 1080\n","\n","    # center points, 2 is the center of the image\n","    points_2d[:, 0] = points_2d[:, 0] + 64 - points_2d[2, 0]\n","    points_2d[:, 1] = points_2d[:, 1] + 64 - points_2d[2, 1]\n","\n","    return points_2d\n","\n","points = project_points(coords, cam)\n","\n","\n","#plot image with points\n","plt.imshow(image.permute(1, 2, 0))\n","plt.scatter(points[:, 0], points[:, 1], c='r')\n","\n","#show number of points\n","for i in range(18):\n","    plt.text(points[i, 0], points[i, 1], str(i), color='r')\n","\n","#connect points 7-8 6-7 2-6 2-12 12-13 13-14 2-0 0-3 0-9 3-4 4-5 9-10 10-11 0-16\n","for connection in connections:\n","    plt.plot(points[connection, 0], points[connection, 1], c='r')\n","\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"19bb1f5c"},"source":["### The loss function that is used for training is L1 loss. As an accuracy metrics I report average euclidean distance between true and predicted joints. The model is also trained for 20 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcqrDPrW0kbu"},"outputs":[],"source":["def find_rotation_mat(points1, points2):\n","    \"\"\"\n","    Function to find the rotation matrix between two sets of ordered 3D points.\n","\n","    Parameters:\n","    - points1: input containing a set of ordered 3D points\n","    - points2: target containing the reference set of ordered 3D points\n","\n","    Returns:\n","    - transformation_matrix: torch.Tensor, 3x3 transformation matrix\n","    \"\"\"\n","\n","    # Calculate the covariance matrix \n","    covariance_matrix = torch.mm(points1.t(), points2)\n","\n","    # Calculate the singular value decomposition\n","    U, S, V = torch.svd(covariance_matrix)\n","\n","    # Calculate the rotation matrix\n","    rotation_matrix = torch.mm(V, U.t())\n","\n","    # special reflection case\n","    if torch.det(rotation_matrix) < 0:\n","        U, S, V = torch.svd(covariance_matrix)\n","        V[2, :] *= -1\n","        rotation_matrix = torch.mm(V, U.t())\n","\n","    return rotation_matrix\n"]},{"cell_type":"code","execution_count":null,"id":"60728a16","metadata":{},"outputs":[],"source":["def find_scaling(points1, points2):\n","    \"\"\"\n","    Function to find the scaling factor between two sets of ordered 3D points.\n","\n","    Parameters:\n","    - points1: input containing a set of ordered 3D points\n","    - points2: target containing the reference set of ordered 3D points\n","\n","    Returns:\n","    - scaling_factor: torch.Tensor, 1x1 scaling factor\n","    \"\"\"\n","    points1 = points1.view(-1)\n","    points2 = points2.view(-1)\n","\n","    # Calculate the scaling factor\n","    scaling_factor = torch.sum(points2 * points1) / torch.sum(points1 * points1)\n","\n","    return scaling_factor"]},{"cell_type":"code","execution_count":null,"id":"335233a7","metadata":{},"outputs":[],"source":["# get loss for the whole batch\n","def get_loss(output, pose, weights=None, norm_factor=0.2):\n","    batch_size = output.shape[0]\n","\n","    # vectors are a column vector and should be grouped by 3 (x, y, z)\n","    output = output.view(batch_size, -1, 3)\n","    pose = pose.view(pose.shape[0], -1, 3)\n","\n","    # add first point to output (0,0,0) for each batch\n","    output = torch.cat((torch.zeros((batch_size, 1, 3)), output), 1)\n","\n","    # center pose on first point for each batch\n","    pose = pose - pose[:, 0].unsqueeze(1)\n","\n","    #find rotation matrix for each batch\n","    batch_rotation_matrix = torch.zeros((batch_size, 3, 3))\n","    # scaling_factor = torch.zeros((batch_size, 1)).to(\"cuda\")\n","\n","    with torch.no_grad():\n","        #center pose on first point for each batch\n","        pose = pose - pose[:, 0].unsqueeze(1)\n","\n","        #print (\"output before\\n\", output)\n","\n","        for i in range(batch_size):\n","            #print(output[i])\n","            rotation_matrix = find_rotation_mat(pose[i], output[i])\n","            \n","            batch_rotation_matrix[i] = rotation_matrix\n","\n","    output = torch.bmm(output, batch_rotation_matrix)\n","\n","    '''\n","    #find scaling factor for each batch\n","\n","    with torch.no_grad():\n","        for i in range(batch_size):\n","            scaling_factor[i] = find_scaling(pose[i], output[i])\n","        \n","    for i in range(batch_size):\n","        output[i] = output[i] * scaling_factor[i].item()\n","    '''\n","\n","    #print (\"output\\n\", output)\n","    #print (\"pose\\n\", pose)\n","    #mean squared error\n","    loss = torch.mean((output - pose)**2)\n","    #print(loss)\n","\n","    # add L2 normalization factor for weights\n","    if weights is not None:\n","        weights = weights.view(-1)\n","        loss += norm_factor * torch.sum(weights**2)\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"id":"e3cf0d47","metadata":{},"outputs":[],"source":["_, _, test_data, _, _, _ = get_data(200)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe8d7a0a"},"outputs":[],"source":["def print_one_example(model, ind):\n","    \"\"\"Function to plot true and predicted joints\"\"\"\n","    image = test_data[ind]['image']\n","    true_coords = test_data[ind]['poses_3d'].reshape((-1,3))\n","\n","    image_path = test_data[ind]['image_path']\n","    print(image_path)\n","\n","    #print(true_coords)\n","    true_coords_center = true_coords - true_coords[0]\n","    #print(true_coords_center)\n","\n","    image_input = image.unsqueeze(0)\n","    mid_outs, pred_coords = model(image_input)\n","\n","    pred_coords = pred_coords[0].reshape((-1,3)).detach()\n","\n","    #add first point to output (0,0,0) for each batch\n","    pred_coords = torch.cat((torch.zeros((1, 3)), pred_coords), 0)\n","    #print(pred_coords)\n","    #find rotation matrix\n","    rotation_matrix = find_rotation_mat(true_coords_center, pred_coords)\n","    #rotate predicted points\n","    pred_coords = torch.matmul(pred_coords, rotation_matrix)\n","\n","\n","    true_coords_2D = project_points(true_coords, test_data[ind]['cam'])\n","    pred_coords_2D = project_points(pred_coords, test_data[ind]['cam'])\n","    \n","\n","    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n","    f.set_figheight(10)\n","    f.set_figwidth(10)\n","    \n","    #plot image with points\n","    ax1.imshow(image.permute(1, 2, 0))\n","    ax1.scatter(true_coords_2D[:, 0], true_coords_2D[:, 1], c='r')\n","\n","    #connect points 7-8 6-7 2-6 2-12 12-13 13-14 2-0 0-3 0-9 3-4 4-5 9-10 10-11 0-16\n","    for connection in connections:\n","        ax1.plot([true_coords_2D[connection[0], 0], true_coords_2D[connection[1], 0]], [true_coords_2D[connection[0], 1], true_coords_2D[connection[1], 1]], c='r')\n","\n","    ax1.set_title(\"True joints\")\n","\n","    ax2.imshow(image.permute(1, 2, 0))\n","\n","    ax2.scatter(pred_coords_2D[:, 0], pred_coords_2D[:, 1], c='r')\n","\n","    #connect points 7-8 6-7 2-6 2-12 12-13 13-14 2-0 0-3 0-9 3-4 4-5 9-10 10-11 0-16\n","    for connection in connections:\n","        ax2.plot([pred_coords_2D[connection[0], 0], pred_coords_2D[connection[1], 0]], [pred_coords_2D[connection[0], 1], pred_coords_2D[connection[1], 1]], c='r')\n","\n","\n","    ax2.set_title(\"Predicted joints\")\n","\n","    plt.show()\n","    \n","    images = {}\n","\n","    for key in mid_outs:\n","        mid_out = mid_outs[key][0].detach()\n","        #show mid_out that is layer1 resnet50 shape (num img,H,W)\n","        #print(mid_out.shape)\n","\n","        num_img = mid_out.shape[0]\n","        sqrt_num_img = int(math.sqrt(num_img))\n","        \n","        num_rows = sqrt_num_img\n","        num_cols = int(math.ceil(num_img / sqrt_num_img))\n","\n","        img_size = mid_out.shape[1]\n","        padding = 1\n","\n","        #print (num_img, sqrt_num_img, img_size)\n","\n","        #normalize each image\n","        for i in range(num_img):\n","            mid_out[i] = mid_out[i] - torch.min(mid_out[i])\n","            mid_out[i] = mid_out[i] / torch.max(mid_out[i])\n","            mid_out[i] = mid_out[i] * 255\n","\n","        img = torch.ones((num_rows*(img_size+padding) + padding, num_cols*(img_size+padding) + padding))\n","        img = img * 255\n","\n","        for i in range(num_img):\n","            row = int(i / num_cols)\n","            col = i % num_cols\n","\n","            img[row*(img_size+padding) + padding:row*(img_size+padding) + padding + img_size, col*(img_size+padding) + padding:col*(img_size+padding) + padding + img_size] = mid_out[i]   \n","\n","        img = img.numpy().astype(np.uint8)\n","\n","        #save image to file\n","        cv2.imwrite(\"mid_out_\"+key+\".png\", img)\n","        \n","        images[key] = img\n","    \n","    #show all mid_outs\n","    f, axarr = plt.subplots(1, len(images))\n","    f.set_figheight(20)\n","    f.set_figwidth(40)\n","    i = 0\n","    for key in images:\n","        axarr[i].imshow(images[key], cmap='gray')\n","        axarr[i].set_title(key)\n","        i += 1\n","    plt.show()\n","    \n"]},{"cell_type":"code","execution_count":null,"id":"b3f24390","metadata":{},"outputs":[],"source":["def show_3d_poses(model, ind):\n","    \"\"\"Function to plot true and predicted joints\"\"\"\n","    image = test_data[ind]['image']\n","    true_coords = test_data[ind]['poses_3d'].reshape((-1,3))\n","\n","    #print(true_coords)\n","    true_coords_center = true_coords - true_coords[0]\n","    #print(true_coords_center)\n","\n","    image_input = image.unsqueeze(0)\n","    pred_coords = model(image_input)[0].reshape((-1,3)).detach()\n","    #add first point to output (0,0,0) for each batch\n","    pred_coords = torch.cat((torch.zeros((1, 3)), pred_coords), 0)\n","    #print(pred_coords)\n","    #find rotation matrix\n","    rotation_matrix = find_rotation_mat(true_coords_center, pred_coords)\n","    #rotate predicted points\n","    rot_pred_coords = torch.matmul(pred_coords, rotation_matrix)\n","\n","    #show 3d scatter plot of true, predicted and rotated predicted points side by side (on different subplots)\n","    fig = plt.figure(figsize=(15, 5))\n","    ax1 = fig.add_subplot(131, projection='3d')\n","    ax1.scatter(true_coords_center[:, 0], true_coords_center[:, 2], true_coords_center[:, 1], c='r')\n","    ax1.set_title(\"True joints\")\n","    ax2 = fig.add_subplot(132, projection='3d')\n","    ax2.scatter(pred_coords[:, 0], pred_coords[:, 2], pred_coords[:, 1], c='r')\n","    ax2.set_title(\"Predicted joints\")\n","    ax3 = fig.add_subplot(133, projection='3d')\n","    ax3.scatter(rot_pred_coords[:, 0], rot_pred_coords[:, 2], rot_pred_coords[:, 1], c='r')\n","    ax3.set_title(\"Rotated predicted joints\")\n","\n","    #keep aspect ratio\n","    ax1.set_aspect('equal')\n","    ax2.set_aspect('equal')\n","    ax3.set_aspect('equal')\n","\n","    #labels\n","    ax1.set_xlabel('X')\n","    ax1.set_ylabel('Y')\n","    ax1.set_zlabel('Z')\n","    ax2.set_xlabel('X')\n","    ax2.set_ylabel('Y')\n","    ax2.set_zlabel('Z')\n","    ax3.set_xlabel('X')\n","    ax3.set_ylabel('Y')\n","    ax3.set_zlabel('Z')\n","\n","    #invert z axis\n","    ax1.invert_zaxis()\n","    ax2.invert_zaxis()\n","    ax3.invert_zaxis()\n","\n","    #invert x axis\n","    ax1.invert_xaxis()\n","    ax2.invert_xaxis()\n","    ax3.invert_xaxis()\n","\n","\n","    #connect points 7-8 6-7 2-6 2-12 12-13 13-14 2-0 0-3 0-9 3-4 4-5 9-10 10-11 0-16\n","    for connection in connections:\n","        ax1.plot([true_coords_center[connection[0], 0], true_coords_center[connection[1], 0]], [true_coords_center[connection[0], 2], true_coords_center[connection[1], 2]], [true_coords_center[connection[0], 1], true_coords_center[connection[1], 1]], c='r')\n","        ax2.plot([pred_coords[connection[0], 0], pred_coords[connection[1], 0]], [pred_coords[connection[0], 2], pred_coords[connection[1], 2]], [pred_coords[connection[0], 1], pred_coords[connection[1], 1]], c='r')\n","        ax3.plot([rot_pred_coords[connection[0], 0], rot_pred_coords[connection[1], 0]], [rot_pred_coords[connection[0], 2], rot_pred_coords[connection[1], 2]], [rot_pred_coords[connection[0], 1], rot_pred_coords[connection[1], 1]], c='r')\n","    \n","    # write loss on plot\n","    loss = get_loss(pred_coords[1:].view(1, -1), true_coords.view(1, -1))\n","    ax1.text2D(0.05, 0.95, \"Loss: {:.4f}\".format(loss), transform=ax1.transAxes)\n","\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f9701dfe","metadata":{},"outputs":[],"source":["#print n worst predictions\n","def print_worst_predictions(model, num):\n","    \"\"\"Function to plot true and predicted joints\"\"\"\n","    #get all predictions\n","    predictions = []\n","    for i in range(len(test_data)):\n","        print(\"\\r {}/{}\".format(i, len(test_data)), end=\"\")\n","        image = test_data[i]['image']\n","        true_coords = test_data[i]['poses_3d'].reshape((-1,3))\n","\n","        #print(true_coords)\n","        true_coords_center = true_coords - true_coords[0]\n","        #print(true_coords_center)\n","\n","        image_input = image.unsqueeze(0)\n","        pred_coords = model(image_input)[0].reshape((-1,3)).detach()\n","\n","        predictions.append(pred_coords)\n","\n","    #calculate loss for each prediction\n","    losses = []\n","    for i in range(len(test_data)):\n","        loss = get_loss(predictions[i].view(1, -1), test_data[i]['poses_3d'].view(1, -1))\n","        losses.append(loss.item())\n","\n","    #sort losses\n","    losses, indices = torch.sort(torch.tensor(losses), descending=True)\n","\n","    #print worst predictions\n","    for i in range(num):\n","        print_one_example(model, indices[i])"]},{"cell_type":"markdown","metadata":{"id":"c5c141ec"},"source":["# 3D pose estimation using SimCLR model\n","\n","The average euclidean distance between true and predicted joints after training is:\n","- Training set: 0.4431\n","- Validation set: 0.4442\n","- Test set: 0.4444"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c02168e8"},"outputs":[],"source":["simclr_path = 'trained_models/ver1.pt'\n","simclr = get_simclr_net()\n","\n","#get last model epoch in trained_models/sim/ folder\n","\n","#get list of files in directory\n","files = os.listdir(\"trained_models/sim\")\n","files = [f for f in files if re.match(r'sim_linear_epoch\\d+.pt', f)]\n","\n","#sort files by epoch number\n","files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n","#get last file\n","path = \"trained_models/sim/\" + files[-1]\n","print(path)\n","\n","#load model\n","\n","simclr_model = get_linear_evaluation_model(simclr_path, simclr, False)\n","simclr_model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","\n","from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter\n","#get intermediate layer of resnet of model and show as images\n","return_layers = {\n","    'conv1': 'conv1',\n","    'layer1': 'layer1',\n","    'layer2': 'layer2',\n","    'layer3': 'layer3',\n","    'layer4': 'layer4',\n","}\n","\n","mid_getter_simclr = MidGetter(simclr_model, return_layers=return_layers, keep_output=True)\n","\n","\n","simclr_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0d3f74fb","outputId":"63e471e0-bded-4a46-a356-d6f8d00f9e6d"},"outputs":[],"source":["print_one_example(mid_getter_simclr, 0)\n","show_3d_poses(simclr_model, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d1746df","outputId":"5806f4a9-b4c7-4e86-81e3-640f7f1f8ee0"},"outputs":[],"source":["print_one_example(mid_getter_simclr, 145)\n","show_3d_poses(simclr_model, 145)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccd96cde","outputId":"db65ee51-9abe-49e8-ded7-d07f64bc2931"},"outputs":[],"source":["print_one_example(mid_getter_simclr, 40)\n","show_3d_poses(simclr_model, 40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f3e44eb","outputId":"3cbdfe93-abbc-4b15-d8f3-3cddd6c2dc7a"},"outputs":[],"source":["print_one_example(mid_getter_simclr, 66)\n","show_3d_poses(simclr_model, 66)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57ef6dc2"},"outputs":[],"source":["for i in range(5):\n","    ind = random.randint(0, len(test_data))\n","    print (\"index\", ind)\n","    print_one_example(mid_getter_simclr, ind)\n","    show_3d_poses(simclr_model, ind)\n","\n","#ProcessedPanopticDataset\\171204_pose2\\hdImages\\171204_pose2;hd_00_20 person 5,6,7 all green"]},{"cell_type":"code","execution_count":null,"id":"0b2d99d2","metadata":{},"outputs":[],"source":["#plot 10 worst predictions\n","#print_worst_predictions(simclr_model, 10)\n"]},{"cell_type":"code","execution_count":null,"id":"487d758b","metadata":{},"outputs":[],"source":["#print conv1 kernels of simclr model\n","params = list(simclr_model.conv1.parameters())\n","print(params[0].size())\n","print(params[0][8][2])\n","print(sum(sum(params[0][8][2])))"]},{"cell_type":"markdown","metadata":{"id":"f31a5e7b"},"source":["# 3D pose estimation using SimSiam model\n","\n","The average euclidean distance between true and predicted joints after training is:\n","- Training set: 0.4075\n","- Validation set: 0.4098\n","- Test set: 0.4111"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d95257c8"},"outputs":[],"source":["siam_path = 'trained_models/siam2.pt'\n","\n","siam = get_siam_net()\n","\n","#get last model epoch in trained_models/siam/ folder\n","files = os.listdir(\"trained_models/siam\")\n","files = [f for f in files if re.match(r'siam_linear_epoch\\d+.pt', f)]\n","\n","#sort files by epoch number\n","files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n","#get last file\n","path = \"trained_models/siam/\" + files[-1]\n","\n","#load model\n","siam_model = get_linear_evaluation_model(siam_path, siam)\n","siam_model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","\n","from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter\n","#get intermediate layer of resnet of model and show as images\n","return_layers = {\n","    'conv1': 'conv1',\n","    'layer1': 'layer1',\n","    'layer2': 'layer2',\n","    'layer3': 'layer3',\n","    'layer4': 'layer4',\n","}\n","\n","mid_getter_siam = MidGetter(siam_model, return_layers=return_layers, keep_output=True)\n","\n","siam_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2130884f","outputId":"6a7a91fb-606e-4497-c7c4-978fff367fa1"},"outputs":[],"source":["print_one_example(mid_getter_siam, 0)\n","show_3d_poses(siam_model, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f03cc3bc","outputId":"ef5f3a87-94d6-4bfb-f29d-9af11a17c55e"},"outputs":[],"source":["print_one_example(mid_getter_siam, 145)\n","show_3d_poses(siam_model, 145)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ed0ebe88","outputId":"6492dd13-4014-459b-930a-b590db1f88fa"},"outputs":[],"source":["print_one_example(mid_getter_siam, 40)\n","show_3d_poses(siam_model, 40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"389ee23b","outputId":"57b5bb57-88c1-4f77-b662-771d57c87087"},"outputs":[],"source":["print_one_example(mid_getter_siam, 66)\n","show_3d_poses(siam_model, 66)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10726850"},"outputs":[],"source":["for i in range(5):\n","    ind = random.randint(0, len(test_data))\n","    print_one_example(mid_getter_siam, ind)\n","    show_3d_poses(siam_model, ind)"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}
