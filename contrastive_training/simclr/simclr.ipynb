{"cells":[{"cell_type":"code","execution_count":null,"id":"1cf51b2b","metadata":{},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter(log_dir=\"trained_models/tensorboard_simclr\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"411eafd1"},"outputs":[],"source":["import os\n","import torch\n","import math\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import json\n","import cv2\n","import re\n","import random\n","import torchvision.transforms as transforms\n","from torchvision.io import read_image\n","\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as T\n","\n","\n","class PanopticDataset(Dataset):\n","    def __init__(self, transform, data_set='training'):\n","\n","        # change this to the path where the dataset is stored\n","        self.data_path = \"ProcessedPanopticDataset\\\\\"\n","        self.training_dir = []\n","\n","        self.transform = transform\n","\n","        self.set = data_set\n","\n","        paths = []\n","\n","        motion_seq = os.listdir(self.data_path)\n","        no_dir = ['scripts','python','matlab','.git','glViewer.py','README.md','matlab',\n","                'README_kinoptic.md', '171204_pose3']\n","\n","        for dir in motion_seq:\n","            if dir not in no_dir:\n","                if 'haggling' in dir:\n","                    continue\n","                elif dir == '171204_pose2' or dir =='171204_pose5' or dir =='171026_cello3':\n","                    if os.path.exists(os.path.join(self.data_path,dir, 'hdJoints')):\n","                        data_path = os.path.join(self.data_path,dir, 'hdJoints')\n","                        for lists in (os.listdir(data_path)):\n","                            paths.append(os.path.join(data_path,lists.split('.json')[0]))\n","                elif 'ian' in dir:\n","                    continue\n","                else:\n","                    if os.path.exists(os.path.join(self.data_path,dir,'hdJoints')):\n","                        data_path = os.path.join(self.data_path,dir,'hdJoints')\n","                        for lists in (os.listdir(data_path)):\n","                            paths.append(os.path.join(data_path,lists.split('.json')[0]))\n","\n","        self.data = {'paths': paths}\n","\n","    def __len__(self):\n","        return len(self.data['paths'])\n","\n","    def get_second_view(self, image_path):\n","        \"\"\"Randomly gets another camera view\"\"\"\n","        split = image_path.split(';')\n","        camera = split[1].split('_')\n","        view = int(camera[-1]) # id of the first view\n","\n","        second_view = random.randint(0, view) if view > 15 else random.randint(view + 1, 30) # randomly get second view id that is smaller or bigger than the first one\n","\n","        camera[2] = str(second_view) if second_view > 9 else '0' + str(second_view)\n","        camera = '_'.join(camera)\n","        split[1] = camera\n","        second_path = ';'.join(split)\n","\n","        return second_path\n","\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        sample = dict()\n","\n","        path_split = self.data['paths'][idx].split('\\\\hdJoints')\n","        image1_path = path_split[0] + '\\\\hdImages' + path_split[-1] + '.jpg'\n","        image2_path = self.get_second_view(image1_path)\n","\n","        for i in range(0, 10):\n","            if os.path.isfile(image2_path):\n","                image2 = cv2.imread(image2_path)\n","                image2 =cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n","                image2 = self.transform(image2)\n","                break\n","            else:\n","                image2_path = self.get_second_view(image1_path)\n","        else:\n","            # apply random rotation on the first image if the second view cannot be found in 10 iterations\n","            image2 = cv2.imread(image1_path)\n","            image2 =cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n","            image2 = self.transform(image2)\n","            image2 = T.RandomRotation(45)(image2)\n","\n","        image1 = cv2.imread(image1_path)\n","        image1 =cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n","        image1 = self.transform(image1)\n","\n","        sample['image1'] = image1\n","        sample['image2'] = image2\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8848169b"},"outputs":[],"source":["def get_dataset(batch_size):\n","    transforms = T.Compose(\n","        [\n","            T.ToTensor(),\n","            T.Resize(size=(128, 128)),\n","        ]\n","    )\n","\n","    dataset = PanopticDataset(transforms)\n","    train_loader = torch.utils.data.DataLoader(dataset, batch_size)\n","\n","    return dataset, train_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8536f9bf"},"outputs":[],"source":["def plot_item(data, ind):\n","    el = data[ind]\n","\n","    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 10))\n","    ax1.imshow(el['image1'].permute(1, 2, 0))\n","    ax2.imshow(el['image2'].permute(1, 2, 0))\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"704ee8ba","outputId":"bae3579f-21fc-435a-9fc6-6566dc8db6df","scrolled":false},"outputs":[],"source":["data, loader = get_dataset(443)\n","plot_item(data, 202)"]},{"cell_type":"markdown","metadata":{"id":"9d7173f7"},"source":["# SimCLR method"]},{"cell_type":"markdown","metadata":{"id":"c6f334d5"},"source":["### Similar to [SimCLR](https://arxiv.org/pdf/2002.05709.pdf) I take the base encoder model (ResNet50) and replace its fully connected layer with a projection head."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26c96dfe"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, out_dim):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, out_dim),\n","        )\n","\n","    def forward(self, x):\n","        return x, self.layers(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15d54262"},"outputs":[],"source":["from torchvision.models import resnet50, ResNet50_Weights\n","\n","\n","def get_simclr_net():\n","    weights = ResNet50_Weights.DEFAULT\n","    model = resnet50(weights=weights)\n","    model.fc = MLP(2048, 2048, 128)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"0442f2c9"},"source":["### I use LARS as an optimizer because SimCLR method works particularly well on large batches where SGD would not work well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a1feb4b","scrolled":true},"outputs":[],"source":["# uncomment this if you get an No module named 'pytorch_lightning.utilities.apply_func' error\n","# !pip3 install pytorch-lightning==1.2.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe956ab1"},"outputs":[],"source":["from flash.core.optimizers import LARS\n","\n","\n","def get_optimizer(model, lr, wd, momentum, epochs):\n","    final_layer_weights = []\n","    rest_of_the_net_weights = []\n","\n","    for name, param in model.named_parameters():\n","        if name.startswith('fc'):\n","            final_layer_weights.append(param)\n","        else:\n","            rest_of_the_net_weights.append(param)\n","\n","    optimizer = LARS([\n","        {'params': rest_of_the_net_weights},\n","        {'params': final_layer_weights, 'lr': lr}\n","    ], lr=lr / 2, weight_decay=wd, momentum=momentum)\n","\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n","\n","    return optimizer, scheduler\n"]},{"cell_type":"markdown","metadata":{"id":"e5dffb25"},"source":["### Implementation of NT-Xent loss (the normalized temperature-scaled cross entropy loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6908d97"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","\n","def get_loss(geom_encoddings, app_encoddings, t):\n","    geom_encoddings = F.normalize(geom_encoddings, p=2, dim=1)\n","    app_encoddings = F.normalize(app_encoddings, p=2, dim=1)\n","\n","    def get_sim(zi, zj, t):\n","        cosi = torch.nn.CosineSimilarity(dim=1)\n","        return torch.exp(cosi(zi, zj) / t)\n","\n","    num = get_sim(geom_encoddings, app_encoddings, t)\n","    num = torch.cat([num, num])\n","\n","    batch = torch.cat([geom_encoddings, app_encoddings])\n","    batch = batch / batch.norm(dim=1)[:, None]\n","    sim = torch.mm(batch, batch.transpose(0,1))\n","    sim = torch.exp(sim / t)\n","\n","    denom = torch.sum(sim, dim=1) - torch.diagonal(sim, 0)\n","    loss_vec = - torch.log(num / denom)\n","\n","    loss = loss_vec.sum() / batch.size()[0]\n","\n","    return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"085b7a30"},"outputs":[],"source":["from tqdm import tqdm\n","\n","\n","def train_step(net, data_loader, optimizer, cost_function, t, device='cuda'):\n","    samples = 0.\n","    cumulative_loss = 0.\n","    net.train()\n","\n","    for batch_idx, batch in enumerate(tqdm(data_loader)):\n","\n","        image1 = batch['image1'].to(device)\n","        image2 = batch['image2'].to(device)\n","\n","        _, image1_encoddings = net(image1)\n","        _, image2_encoddings = net(image2)\n","\n","        loss = cost_function(image1_encoddings, image2_encoddings, t)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        optimizer.zero_grad()\n","\n","        cumulative_loss += loss.item()\n","        samples += image1.shape[0]\n","\n","    return cumulative_loss / samples\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6e10567"},"outputs":[],"source":["def main(batch_size=1024, device='cuda:0', learning_rate=0.01, weight_decay=0.000001, momentum=0.9, t=0.6, epochs=100):\n","    _, train_loader = get_dataset(batch_size)\n","\n","    net = get_simclr_net()\n","    net.to(device)\n","\n","    optimizer, scheduler = get_optimizer(net, lr=learning_rate, wd=weight_decay, momentum=momentum, epochs=epochs)\n","\n","    cost_function = get_loss\n","\n","    #get latest epoch\n","    epoch = 0\n","    for file in os.listdir('trained_models/simclr_new'):\n","        if 'simclr_epoch' in file:\n","            e = int(re.findall(r'\\d+', file)[0])\n","            if e > epoch:\n","                epoch = e\n","\n","    print ('Starting training from epoch {:d}'.format(epoch))\n","\n","    #load latest model\n","    if epoch > 0:\n","        net.load_state_dict(torch.load('trained_models/simclr_new/simclr_epoch_{:d}.pt'.format(epoch)))\n","        #load optimizer\n","        optimizer.load_state_dict(torch.load('trained_models/simclr_new/simclr_epoch_{:d}_optimizer.pt'.format(epoch)))\n","        #load scheduler\n","        scheduler.load_state_dict(torch.load('trained_models/simclr_new/simclr_epoch_{:d}_scheduler.pt'.format(epoch)))\n","\n","    for e in range(epoch, epochs):\n","        train_loss = train_step(net, train_loader, optimizer, cost_function, t, device)\n","\n","        scheduler.step()\n","\n","        print('Epoch: {:d}'.format(e+1))\n","        print('\\tTraining loss {:.5f}'.format(train_loss))\n","\n","        writer.add_scalar(\"simclr/loss\", train_loss, e+1) \n","        writer.add_scalar(\"simclr/lr\", scheduler.get_last_lr()[0], e+1) \n","        writer.flush()\n","\n","        torch.save(net.state_dict(), 'trained_models/simclr_new/simclr_epoch_{:d}.pt'.format(e+1))\n","        torch.save(optimizer.state_dict(), 'trained_models/simclr_new/simclr_epoch_{:d}_optimizer.pt'.format(e+1))\n","        torch.save(scheduler.state_dict(), 'trained_models/simclr_new/simclr_epoch_{:d}_scheduler.pt'.format(e+1))\n"]},{"cell_type":"markdown","metadata":{"id":"f21b57b5"},"source":["### Due to the memory constraint, I use batch size of 200, however, larger batch size is preferred. The training was done on 20 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"455c7858","scrolled":true},"outputs":[],"source":["main(batch_size=180, epochs=100, learning_rate=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8facd88"},"outputs":[],"source":["writer.close()"]},{"cell_type":"markdown","metadata":{"id":"1851fe11"},"source":["# Clustering"]},{"cell_type":"markdown","metadata":{"id":"3cc5e1e5"},"source":["For clustering I use the dataset of 6000 images that contains different poses. I extract the representations from the trained model and run K means clustering algorithm (number of clusters=8) on these representation. I reduce the dimentionality of the representations with PCA algorithm and plot the clustering. For the comparison I use the representations extracted from encoder model (before the projection head) and the representations extracted after the projection head."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8b62029d"},"outputs":[],"source":["class ClusterDataset(Dataset):\n","    def __init__(self, transform, data_set='training'):\n","\n","        # change this to the path where the dataset is stored\n","        self.data_path = \"ProcessedPanopticDataset/171204_pose3/hdImages\"\n","\n","        images = [os.path.join(self.data_path, f) for f in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, f))][0:6000]\n","        self.transform = transform\n","\n","        self.data = {'paths': images}\n","\n","\n","    def __len__(self):\n","        return len(self.data['paths'])\n","\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        sample = dict()\n","\n","        image_path = self.data['paths'][idx]\n","\n","        image = cv2.imread(image_path)\n","        image =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = self.transform(image)\n","\n","        sample['image'] = image\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1a0a90d"},"outputs":[],"source":["def get_cluster_data(batch_size):\n","    transforms = T.Compose(\n","        [\n","            T.ToTensor(),\n","            T.Resize(size=(128, 128)),\n","        ]\n","    )\n","\n","    cluster_dataset = ClusterDataset(transforms)\n","    cluster_loader = torch.utils.data.DataLoader(cluster_dataset, batch_size)\n","\n","    return cluster_dataset, cluster_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80b3b102"},"outputs":[],"source":["def extract_representations(path, cluster_loader, load=True):\n","    net = get_simclr_net()\n","\n","    if load:\n","        net.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n","\n","    net.to('cpu')\n","    net.eval()\n","\n","    proj_repr = []\n","    base_repr = []\n","\n","    with torch.no_grad():\n","        for batch_idx, inputs in enumerate(cluster_loader):\n","            images = inputs['image']\n","            images.to('cpu')\n","            base, proj = net(images)\n","            proj_repr.append(proj)\n","            base_repr.append(base)\n","\n","    return torch.cat(base_repr).numpy(), torch.cat(proj_repr).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22307fb9"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","\n","def kmeans_algorithm(features, n_clusters=8):\n","    kmeans = KMeans(n_clusters=n_clusters)\n","    kmeans.fit(features)\n","\n","    return kmeans.labels_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"614ae399"},"outputs":[],"source":["#import LDA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","\n","def reduce_dim(features, labels):\n","    lda = LDA(n_components=2)\n","    lda.fit(features, labels)\n","\n","    return lda.transform(features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26f9bb8d"},"outputs":[],"source":["def plot_clusters(clusters, features, title):\n","    colors = {\n","        0: '#F8512E', 1: '#F8F82E',\n","        2: '#40F82E', 3: '#2EC1F8',\n","        4: '#6B2EF8', 5: '#D92EF8',\n","        6: '#731642', 7: '#092040'\n","    }\n","\n","    cluster_colors = [colors[c] for c in clusters]\n","\n","    plt.scatter(features[:, 0], features[:, 1], c=cluster_colors)\n","    plt.title(title)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51fa29d4"},"outputs":[],"source":["from sklearn.metrics import silhouette_score\n","\n","\n","def cluster(model_path, load=True):\n","    cluster_set, cluster_loader = get_cluster_data(30)\n","\n","    base_features, proj_features = extract_representations(model_path, cluster_loader, load)\n","\n","    labels_base = kmeans_algorithm(base_features)\n","    lda_base = reduce_dim(base_features, labels_base)\n","\n","    labels_proj = kmeans_algorithm(proj_features)\n","    lda_proj = reduce_dim(proj_features, labels_proj)\n","\n","    silhouette_base = silhouette_score(base_features, labels_base)\n","    silhouette_proj = silhouette_score(proj_features, labels_proj)\n","\n","    print(\"Silhouette score for the encoder features: {}\".format(silhouette_base))\n","    print(\"Silhouette score for the projection head features: {}\".format(silhouette_proj))\n","\n","    plot_clusters(labels_base, lda_base, \"Encoder features\")\n","    plot_clusters(labels_proj, lda_proj, \"Projection head features\")\n","\n","    return cluster_set, base_features, proj_features, labels_base, labels_proj, lda_base, lda_proj"]},{"cell_type":"markdown","metadata":{"id":"1ac3d405"},"source":["## PCA visualization based on the model preprained with 0.0001 learning rate for base encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b24e4d09","outputId":"cd2200ba-ecf0-4f0b-9f58-60ebcc62767e"},"outputs":[],"source":["#get the latest model\n","path = 'trained_models/simclr/'\n","epoch = 0\n","for file in os.listdir(path):\n","    if 'simclr_epoch' in file:\n","        e = int(re.findall(r'\\d+', file)[0])\n","        if e > epoch:\n","            epoch = e\n","\n","path = path + 'simclr_epoch_{:d}.pt'.format(epoch)\n","\n","#path=\"trained_models/ver1.pt\"\n","\n","cluster_set, base_features, proj_features, labels_base, labels_proj, pca_base, pca_proj = cluster(path)"]},{"cell_type":"markdown","metadata":{"id":"3024983d"},"source":["## PCA visualization before training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edb3de48","outputId":"a40cee51-c5e6-4ed5-e745-328faef6592c","scrolled":false},"outputs":[],"source":["_, _, _, _, _, _, _ = cluster(path, False)"]},{"cell_type":"markdown","metadata":{"id":"24516d03"},"source":["## Clustering results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bf44567b"},"outputs":[],"source":["def plot_one_cluster(dataset, cluster_ind, labels):\n","    indexes = [i for i in range(0, len(labels)) if labels[i]==cluster_ind]\n","\n","    plt.figure(figsize = (10,10))\n","\n","    for i in range(0, 9):\n","        ax = plt.subplot(3, 3, i+1)\n","        ax.imshow(dataset[indexes[random.randint(0, len(indexes)-1)]]['image'].permute(1, 2, 0))\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"232accfc"},"source":["## Clustering based on features extracted from base encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2460064e","outputId":"5fc47793-d503-436c-d134-aab81e67fa47","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 0, labels_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaf095ba","outputId":"3fda7706-188e-4b17-b585-12cf5951e974","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 1, labels_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"456efc72","outputId":"5c7be5e7-e0cb-4c5c-a352-d01e95a6da19","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 2, labels_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68a91c53","outputId":"207fdc80-d1fe-45c1-fc38-785245fb60f8","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 3, labels_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"704ceaf0","outputId":"606cf3e9-c838-4fc8-f178-45363fe6b864","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 4, labels_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e819374","outputId":"7323525c-6d1e-4161-f68a-7e9c77b057af","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 5, labels_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3635447a","outputId":"d2d18b71-053d-4038-e83e-1964e46c5e47","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 6, labels_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"714a3cd8","outputId":"58cc722e-ce61-44e5-9f7d-2089a3d46ce8","scrolled":false},"outputs":[],"source":["plot_one_cluster(cluster_set, 7, labels_base)"]},{"cell_type":"markdown","metadata":{"id":"b22d0eae"},"source":["## Clustering based on features extracted from projection head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23b89d8c","outputId":"c0e9f70d-7704-488e-ad4a-8222394b714e"},"outputs":[],"source":["plot_one_cluster(cluster_set, 0, labels_proj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8380f483","outputId":"2c05fa62-64aa-4e41-d150-0586cff400cc"},"outputs":[],"source":["plot_one_cluster(cluster_set, 1, labels_proj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68a640b7","outputId":"000fdd8c-adec-4831-88bb-aaec6eb471bd"},"outputs":[],"source":["plot_one_cluster(cluster_set, 2, labels_proj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22694c12","outputId":"3e7b6b4f-3228-4875-df30-c12c6c093dbc"},"outputs":[],"source":["plot_one_cluster(cluster_set, 3, labels_proj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a71fdd0c","outputId":"52c4f152-66b4-4a49-b930-628bc8009c95"},"outputs":[],"source":["plot_one_cluster(cluster_set, 4, labels_proj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4928f3d","outputId":"2e9e5464-623b-4c27-d7f4-292019619355"},"outputs":[],"source":["plot_one_cluster(cluster_set, 5, labels_proj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84893076","outputId":"89062524-764b-4ab5-c9cf-77daf189a7fd"},"outputs":[],"source":["plot_one_cluster(cluster_set, 6, labels_proj)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dba67919","outputId":"f012ac62-559c-4dc4-927b-b529f80e89bb"},"outputs":[],"source":["plot_one_cluster(cluster_set, 7, labels_proj)"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}
