{"cells":[{"cell_type":"code","execution_count":null,"id":"9a8784ab","metadata":{},"outputs":[],"source":["connections = [(7,8),(6,7),(2,6),(2,12),(12,13),(13,14),(2,0),(0,3),(0,9),(3,4),(4,5),(9,10),(10,11),(0,16)]"]},{"cell_type":"code","execution_count":null,"id":"09542507","metadata":{},"outputs":[],"source":["from torchvision.models import resnet50, ResNet50_Weights\n","import torch.nn as nn\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, out_dim):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, out_dim),\n","        )\n","\n","    def forward(self, x):\n","        return x, self.layers(x)\n","\n","def get_simclr_net():\n","    weights = ResNet50_Weights.DEFAULT\n","    model = resnet50(weights=weights)\n","    model.fc = MLP(2048, 2048, 128)\n","\n","    return model\n","\n","class Projector(nn.Module):\n","    def __init__(self, input_dim, out_dim, hidder_proj):\n","        super().__init__()\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_dim, hidder_proj),\n","            nn.BatchNorm1d(hidder_proj),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(hidder_proj, hidder_proj),\n","            nn.BatchNorm1d(hidder_proj),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(hidder_proj, out_dim),\n","        )\n","\n","    def forward(self, x):\n","        return x, self.proj(x)\n","\n","class SiamMLP(nn.Module):\n","    def __init__(self, base, input_dim, out_dim, hidder_proj, hidden_pred):\n","        super().__init__()\n","\n","        self.base = base\n","\n","        base.fc = Projector(input_dim, out_dim, hidder_proj)\n","\n","        self.predictor = nn.Sequential(\n","            nn.Linear(out_dim, hidden_pred),\n","            nn.BatchNorm1d(hidden_pred),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(hidden_pred, out_dim)\n","        )\n","\n","\n","    def forward(self, x):\n","        x, projections =  self.base(x)\n","\n","        predictions = self.predictor(projections)\n","\n","        return x, projections.detach(), predictions\n","\n","def get_siam_net():\n","    weights = ResNet50_Weights.DEFAULT\n","    model = resnet50(weights=weights)\n","    model = SiamMLP(model, 2048, 2048, 2048, 512)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1VYaEOcUoGcVV-4WXfc-uivNn0_i4hLYh"},"id":"2fb45b47","outputId":"f9b5a508-3060-4708-b126-40035c0c677a"},"outputs":[],"source":["import os\n","import torch\n","import math\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import json\n","import cv2\n","import re\n","import random\n","import torchvision.transforms as transforms\n","from torchvision.io import read_image\n","\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as T\n","\n","\n","\n","class PanopticDataset(Dataset):\n","    def __init__(self, transform):\n","\n","        # change this to the path where the dataset is stored\n","        self.data_path = \"ProcessedPanopticDataset\\\\\"\n","        self.training_dir = []\n","\n","        self.transform = transform\n","\n","        paths = []\n","\n","        motion_seq = os.listdir(self.data_path)\n","        no_dir = ['scripts','python','matlab','.git','glViewer.py','README.md','matlab',\n","                'README_kinoptic.md']\n","\n","        for dir in motion_seq:\n","            if dir not in no_dir:\n","                if 'haggling' in dir:\n","                    continue\n","                elif dir == '171204_pose2' or dir =='171204_pose5' or dir =='171026_cello3':\n","                    if os.path.exists(os.path.join(self.data_path,dir,'hdJoints')):\n","                        data_path = os.path.join(self.data_path,dir,'hdJoints')\n","                        for lists in (os.listdir(data_path)):\n","                            paths.append(os.path.join(data_path,lists.split('.json')[0]))\n","                elif 'ian' in dir:\n","                    continue\n","                else:\n","                    if os.path.exists(os.path.join(self.data_path,dir,'hdJoints')):\n","                        data_path = os.path.join(self.data_path,dir,'hdJoints')\n","                        for lists in (os.listdir(data_path)):\n","                            \n","                            paths.append(os.path.join(data_path,lists.split('.json')[0]))\n","\n","        self.data = {'paths': paths}\n","\n","    def __len__(self):\n","        return len(self.data['paths'])\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        sample = dict()\n","\n","        path_split = self.data['paths'][idx].split('\\\\hdJoints')\n","        image_path = path_split[0] + '\\\\hdImages' + path_split[-1] + '.jpg'\n","\n","        image = cv2.imread(image_path)\n","        image =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = self.transform(image)\n","\n","        sample['image'] = image\n","\n","        joints_path = self.data['paths'][idx]+'.json'\n","\n","        with open(joints_path) as dfile:\n","            bframe = json.load(dfile)\n","\n","        poses_3d = torch.tensor(np.array(bframe['poses_3d']), dtype=torch.float32)\n","        #remove every 4th element of one dimensional array poses_3d\n","        poses_3d = poses_3d.reshape(-1, 4)[:, :3].reshape(-1)\n","\n","        sample['poses_3d'] =  poses_3d\n","\n","        cam = bframe['cam']\n","        # Replace 'array' with 'list' in the cam string\n","        cam = cam.replace('array', 'list')\n","\n","        sample['cam'] = cam\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71ffc15a"},"outputs":[],"source":["# uncomment this if you get an No module named 'pytorch_lightning.utilities.apply_func' error\n","# !pip3 install pytorch-lightning==1.2.2"]},{"cell_type":"markdown","metadata":{"id":"abacd64a"},"source":["### On top of a base encoder, a simple linear layer is attached that predicts 19 pairs of joints. The base encoder can be either SimCLR or Siamese models. I remove the projection head (or projection and prediction in case of Siamese) and use only the output of the base encoder, which is the output of AvrPool layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76e123aa"},"outputs":[],"source":["class Linear(nn.Module):\n","    def __init__(self):\n","        super(Linear, self).__init__()\n","        self.layers = nn.Sequential(nn.Linear(2048, 128), nn.ReLU(), nn.Linear(128, 18*3))\n","\n","\n","    def forward(self, x):\n","        z = self.layers(x)\n","        return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d379cd49"},"outputs":[],"source":["def get_linear_evaluation_model(path, base, siam=True):\n","\n","    base.load_state_dict(torch.load(path, map_location=torch.device('cuda')))\n","\n","    if siam:\n","        base = base.base\n","        base.fc = Linear()\n","    else:\n","        base.fc = Linear()\n","\n","    return base"]},{"cell_type":"markdown","metadata":{"id":"9780c064"},"source":["### I freeze all the layers of the base encoder and use Stochastic gradient descent to train only the linear layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2ff689e"},"outputs":[],"source":["from torch.optim import SGD\n","from torch.optim import Adam\n","\n","\n","def get_optimizer(net, learning_rate, weight_decay, momentum):\n","    final_layer_weights = []\n","    rest_of_the_net_weights = []\n","\n","    for name, param in net.named_parameters():\n","        #if from linear layer then add to final_layer_weights otherwise set requires_grad to false\n","        if 'fc' in name:\n","            final_layer_weights.append(param)\n","        else:\n","            rest_of_the_net_weights.append(param)\n","            param.requires_grad = False\n","        \n","    #print (final_layer_weights)\n","\n","    optimizer = Adam(final_layer_weights, weight_decay=weight_decay)\n","    #optimizer = SGD([ {'params': final_layer_weights, 'lr': learning_rate} ], weight_decay=weight_decay, momentum=momentum)\n","\n","    return optimizer"]},{"cell_type":"markdown","metadata":{"id":"d8e81d26"},"source":["### The images are resized into (128, 128) since the models were trained using this size. The dataset is split into 60%/20%/20% for training, validation and testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3e436a09"},"outputs":[],"source":["generator = torch.Generator().manual_seed(42)\n","\n","def get_data(batch_size):\n","\n","    transforms = T.Compose(\n","        [\n","            T.ToTensor(),\n","            T.Resize(size=(128, 128)),\n","        ]\n","    )\n","\n","    data = PanopticDataset(transforms)\n","\n","    num_samples = len(data)\n","\n","    training_samples = int(num_samples * 0.6 + 1)\n","    val_samples = int(num_samples * 0.2 + 1)\n","    test_samples = num_samples - training_samples - val_samples\n","\n","    training_data, val_data, test_data = torch.utils.data.random_split(\n","        data, [training_samples, val_samples, test_samples], generator=generator\n","    )\n","\n","    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n","\n","    return training_data, val_data, test_data, train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a423fabc"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Assuming your get_data function returns appropriate data\n","training_data, _, _, _, _, _ = get_data(201)\n","\n","image = training_data[9]['image']\n","coords = training_data[9]['poses_3d'].reshape((-1, 3))\n","cam = training_data[9]['cam']\n","\n","\n","def project_points( points_3d, camera_parameters_str):\n","    # Load camera parameters\n","    camera_params = eval(camera_parameters_str)\n","\n","    # Camera intrinsic matrix\n","    K = camera_params['K']\n","\n","    # Rotation matrix and translation vector\n","    R = camera_params['R']\n","    t = camera_params['t']\n","\n","    # Projection matrix\n","    P = np.dot(K, np.hstack((R, t)))\n","\n","    # Homogeneous 3D points\n","    points_3d_homogeneous = np.hstack((points_3d, np.ones((points_3d.shape[0], 1))))\n","\n","    # Project 3D points to 2D\n","    points_2d_homogeneous = np.dot(P, points_3d_homogeneous.T).T\n","\n","    # Normalize homogeneous coordinates\n","    points_2d = points_2d_homogeneous[:, :2] / points_2d_homogeneous[:, 2:]\n","\n","    # rseize to 128x128\n","    points_2d[:, 0] = points_2d[:, 0] * 128 / 1080\n","    points_2d[:, 1] = points_2d[:, 1] * 128 / 1080\n","\n","    # center points, 2 is the center of the image\n","    points_2d[:, 0] = points_2d[:, 0] + 64 - points_2d[2, 0]\n","    points_2d[:, 1] = points_2d[:, 1] + 64 - points_2d[2, 1]\n","\n","    return points_2d\n","\n","points = project_points(coords, cam)\n","\n","\n","#plot image with points\n","plt.imshow(image.permute(1, 2, 0))\n","plt.scatter(points[:, 0], points[:, 1], c='r')\n","\n","#show number of points\n","for i in range(18):\n","    plt.text(points[i, 0], points[i, 1], str(i), color='r')\n","\n","#connect points 7-8 6-7 2-6 2-12 12-13 13-14 2-0 0-3 0-9 3-4 4-5 9-10 10-11 0-16\n","for connection in connections:\n","    plt.plot(points[connection, 0], points[connection, 1], c='r')\n","\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"19bb1f5c"},"source":["### The loss function that is used for training is L2 loss. As an accuracy metrics I report average euclidean distance between true and predicted joints. The model is also trained for 20 epochs."]},{"cell_type":"code","execution_count":null,"id":"695eea04","metadata":{},"outputs":[],"source":["def find_rotation_mat(points1, points2):\n","    \"\"\"\n","    Function to find the rotation matrix between two sets of ordered 3D points.\n","\n","    Parameters:\n","    - points1: input containing a set of ordered 3D points\n","    - points2: target containing the reference set of ordered 3D points\n","\n","    Returns:\n","    - transformation_matrix: torch.Tensor, 3x3 transformation matrix\n","    \"\"\"\n","\n","    # Calculate the covariance matrix \n","    covariance_matrix = torch.mm(points1.t(), points2)\n","\n","    # Calculate the singular value decomposition\n","    U, S, V = torch.svd(covariance_matrix)\n","\n","    # Calculate the rotation matrix\n","    rotation_matrix = torch.mm(V, U.t())\n","\n","    # special reflection case\n","    if torch.det(rotation_matrix) < 0:\n","        U, S, V = torch.svd(covariance_matrix)\n","        V[2, :] *= -1\n","        rotation_matrix = torch.mm(V, U.t())\n","\n","    return rotation_matrix"]},{"cell_type":"code","execution_count":null,"id":"834e18d7","metadata":{},"outputs":[],"source":["def find_scaling(points1, points2):\n","    \"\"\"\n","    Function to find the scaling factor between two sets of ordered 3D points.\n","\n","    Parameters:\n","    - points1: input containing a set of ordered 3D points\n","    - points2: target containing the reference set of ordered 3D points\n","\n","    Returns:\n","    - scaling_factor: torch.Tensor, 1x1 scaling factor\n","    \"\"\"\n","    points1 = points1.view(-1)\n","    points2 = points2.view(-1)\n","\n","    # Calculate the scaling factor\n","    scaling_factor = torch.sum(points2 * points1) / torch.sum(points1 * points1)\n","\n","    return scaling_factor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcqrDPrW0kbu"},"outputs":[],"source":["# get loss for the whole batch\n","def get_loss(output, pose, weights=None, norm_factor=0.2):\n","    batch_size = output.shape[0]\n","\n","    # vectors are a column vector and should be grouped by 3 (x, y, z)\n","    output = output.view(batch_size, -1, 3)\n","    pose = pose.view(pose.shape[0], -1, 3)\n","\n","    # add first point to output (0,0,0) for each batch\n","    output = torch.cat((torch.zeros((batch_size, 1, 3)).to(\"cuda\"), output), 1)\n","\n","    # center pose on first point for each batch\n","    pose = pose - pose[:, 0].unsqueeze(1)\n","\n","    #find rotation matrix for each batch\n","    batch_rotation_matrix = torch.zeros((batch_size, 3, 3)).to(\"cuda\")\n","    # scaling_factor = torch.zeros((batch_size, 1)).to(\"cuda\")\n","\n","    with torch.no_grad():\n","        #center pose on first point for each batch\n","        pose = pose - pose[:, 0].unsqueeze(1)\n","\n","        #print (\"output before\\n\", output)\n","\n","        for i in range(batch_size):\n","            #print(output[i])\n","            rotation_matrix = find_rotation_mat(pose[i], output[i])\n","            \n","            batch_rotation_matrix[i] = rotation_matrix\n","\n","    output = torch.bmm(output, batch_rotation_matrix)\n","\n","    '''\n","    #find scaling factor for each batch\n","\n","    with torch.no_grad():\n","        for i in range(batch_size):\n","            scaling_factor[i] = find_scaling(pose[i], output[i])\n","        \n","    for i in range(batch_size):\n","        output[i] = output[i] * scaling_factor[i].item()\n","    '''\n","\n","    #print (\"output\\n\", output)\n","    #print (\"pose\\n\", pose)\n","    #mean squared error for each batch\n","    loss = torch.mean((pose - output)**4)\n","    #print(loss)\n","\n","    # add L2 normalization factor for weights\n","    if weights is not None:\n","        weights = weights.view(-1)\n","        loss += norm_factor * torch.sum(weights**2)\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1099198a"},"outputs":[],"source":["from tqdm import tqdm\n","\n","\n","\n","def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n","    batches = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","    samples = 0.0\n","\n","    net.train()\n","\n","    for batch_idx, batch in enumerate(tqdm(data_loader)):\n","\n","        images = batch['image']\n","        poses = batch['poses_3d']\n","\n","        images = images.to(device)\n","        poses = poses.to(device)\n","\n","        output = net(images)\n","\n","        loss = cost_function(output, poses)\n","        cumulative_loss += loss.item()\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        optimizer.zero_grad()\n","\n","        batches += 1\n","        samples += images.shape[0]\n","\n","        cumulative_accuracy += torch.cdist(output, poses[:,3:], 2).mean()\n","\n","    return cumulative_loss / batches, cumulative_accuracy / samples\n","\n","\n","def test_step(net, data_loader, cost_function, device='cuda'):\n","    batches = 0.\n","    cumulative_loss = 0.\n","    cumulative_accuracy = 0.\n","    samples = 0.\n","\n","    net.eval()\n","\n","    with torch.no_grad():\n","\n","        for batch_idx, batch in enumerate(tqdm(data_loader)):\n","            images = batch['image']\n","            poses = batch['poses_3d']\n","\n","            images = images.to(device)\n","            poses = poses.to(device)\n","\n","            output = net(images)\n","\n","            loss = cost_function(output, poses)\n","            cumulative_loss += loss.item()\n","\n","            batches += 1\n","            samples += images.shape[0]\n","            cumulative_accuracy += torch.cdist(output, poses[:,3:], 2).mean()\n","\n","    return cumulative_loss / batches, cumulative_accuracy / samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d644129"},"outputs":[],"source":["def main(path, base, batch_size=128, device='cuda', learning_rate=0.05, weight_decay=0.000001, momentum=0.9, epochs=20, siam=False):\n","    _, _, _, train_loader, val_loader, test_loader = get_data(batch_size)\n","\n","    net = get_linear_evaluation_model(path, base, siam=siam).to(device)\n","    epoch = 0\n","\n","    if (siam):\n","        info_file = \"trained_models/siam/siam_linear.txt\"\n","        model_file = \"trained_models/siam/siam_linear_epoch\"\n","        model_dir = \"trained_models/siam\"\n","        optimizer_file = \"trained_models/siam/siam_linear_optimizer.pt\"\n","    else:\n","        info_file = \"trained_models/sim/sim_linear.txt\"\n","        model_file = \"trained_models/sim/sim_linear_epoch\"\n","        model_dir = \"trained_models/sim\"\n","        optimizer_file = \"trained_models/sim/sim_linear_optimizer_epoch\"\n","\n","    #load weights\n","    if os.path.exists(model_dir):\n","        files = os.listdir(model_dir)\n","        #remove .txt file\n","        if len(files) > 1:\n","            files.remove(info_file.split('/')[-1])\n","            epoch = max([int(re.findall(r'\\d+', file)[0]) for file in files])\n","            net.load_state_dict(torch.load(model_file+str(epoch)+'.pt', map_location=torch.device('cuda')))\n","            print(\"Loaded weights from epoch\", epoch)\n","        else:\n","            print(\"No weights found\")\n","            f = open(info_file, \"w\")\n","            f.close()\n","\n","        \n","    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","    if os.path.exists(optimizer_file+str(epoch)+'.pt'):\n","        print(\"Loaded optimizer from epoch\", epoch)\n","        optimizer.load_state_dict(torch.load(optimizer_file+str(epoch)+'.pt'))\n","\n","    cost_function = get_loss\n","\n","    for e in range(epoch, epochs):\n","\n","        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n","        val_loss, val_accuracy = test_step(net, val_loader, cost_function, device)\n","\n","        print('Epoch: {:d}'.format(e+1))\n","        print('\\tTraining loss {:.5f}, Training Acc {:.4f}'.format(train_loss, train_accuracy))\n","        print('\\tValidation loss {:.5f}, Validation Acc {:.2f}'.format(val_loss, val_accuracy))\n","        print('-----------------------------------------------------')\n","\n","        torch.save(net.state_dict(), model_file+str(e+1)+'.pt')\n","        torch.save(optimizer.state_dict(), optimizer_file+str(e+1)+'.pt')\n","        #write information to file\n","        f = open(info_file, \"a\")\n","        f.write('Epoch: {:d}\\n'.format(e+1))\n","        f.write('\\tTraining loss {:.5f}, Training Acc {:.4f}\\n'.format(train_loss, train_accuracy))\n","        f.write('\\tValidation loss {:.5f}, Validation Acc {:.2f}\\n'.format(val_loss, val_accuracy))\n","        f.write('-----------------------------------------------------\\n')\n","        f.close()\n","\n","    print('After training:')\n","    train_loss, train_accuracy = test_step(net, train_loader, cost_function, device)\n","    val_loss, val_accuracy = test_step(net, val_loader, cost_function, device)\n","    test_loss, test_accuracy = test_step(net, test_loader, cost_function, device)\n","\n","    print('\\tTraining loss {:.5f}, Training Acc {:.4f}'.format(train_loss, train_accuracy))\n","    print('\\tValidation loss {:.5f}, Validation Acc {:.4f}'.format(val_loss, val_accuracy))\n","    print('\\tTest loss {:.5f}, Test Acc {:.4f}'.format(test_loss, test_accuracy))\n","    print('-----------------------------------------------------')\n","\n","    #write information to file\n","    f = open(info_file, \"a\")\n","    f.write('After training:\\n')\n","    f.write('\\tTraining loss {:.5f}, Training Acc {:.4f}\\n'.format(train_loss, train_accuracy))\n","    f.write('\\tValidation loss {:.5f}, Validation Acc {:.4f}\\n'.format(val_loss, val_accuracy))\n","    f.write('\\tTest loss {:.5f}, Test Acc {:.4f}\\n'.format(test_loss, test_accuracy))\n","    f.write('-----------------------------------------------------\\n')\n","    f.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":4441,"status":"error","timestamp":1705506948499,"user":{"displayName":"Davide Cavicchini","userId":"03391175637593717529"},"user_tz":-60},"id":"29ac7ac1","outputId":"8aa16d94-1a2f-45b7-fd39-37afd39c06c6"},"outputs":[],"source":["simclr_path = 'trained_models/ver1.pt'\n","\n","simclr = get_simclr_net()\n","\n","main(simclr_path, simclr, epochs=20, siam=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3260b7c7"},"outputs":[],"source":["siam_path = 'trained_models/siam2.pt'\n","\n","siam = get_siam_net()\n","\n","main(siam_path, siam, epochs=20, siam=True)"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}
